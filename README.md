# Topical Terms

Pyspark pipeline to gather statistics on word usage in more than 1500 different topics by analyzing the conversations occurring within topic-specific communities on Reddit.

## Overview
Reddit comments in JSON format are ingested from the PushShift Reddit API and stored on S3 for distributed consumption by a Spark cluster.

In Spark, the comments are classified by topic (the list of topics are from the subreddit_topics.csv generated by the subreddit_topics.py script included in this repo) cleaned and split into individual words. Then, each word's daily frequency and 5-day rolling average frequency are determined for every topic.

These results are stored as a table `topical_terms` with the following schema:

```
    change_in_average_of_rate_in_topic: double
    daily_word_frequency: bigint
    daily_word_frequency_in_topic: bigint
    date: string
    id (pk): string
    topic: string
    topic_daily_word_count: bigint
    rate: double
    rate_in_topic: double
    five_day_average_of_rate_in_topic: double
    topic_specificity: double
    total_daily_word_count: bigint
    word: string
```

This table supports queries to determine words with high specificity (`topic_specificity`) and increasing usage(`five_day_average_of_rate_in_topic`)

## Installation
1. Clone this repo
2. Using poetry run `poetry install`

## Building local-dev image
1. Install docker
2. From the pyspark-pipeline directory (the same one containing this README) Run
```
docker build -t pyspark-pipeline:latest -f docker/local-developer-image/Dockerfile .
```

## Running Tests

`poetry run tox`

## Executing Pipeline

### Client Mode
```
python -m topical_terms.topical_terms -s settings/topical_terms.yaml
```

### Cluster Mode
```
scripts/cluster_submit.sh -f src/topical_terms/topical_terms.py -s settings/topical_terms.yaml -m yarn
```
